% !Mode:: "TeX:UTF-8"

\chapter[基于图神经网络的语义依存图应用]{基于图神经网络的语义依存图应用}[Semantic Dependency Graph Application Based on Graph Neural Networks]

\section{引言}[Introduction]
\label{sec:chapter5-intro}

以句法依存树为代表的结构化信息，在传统的神经网络模型中被广泛应用于句子信息的获取。
例如，Socher等人\cite{socher-etal-2011-parsing}在2011年提出了递归神经网络，利用句法结构对文本进行编码。
随后在2015年，Tai等人在此基础上提出了Tree-LSTM，用于解决递归神经网络训练过程中的梯度消失和梯度爆炸问题。
随后一系列图神经网络\cite{kipf-welling-2017-semi, velickovic-etal-2018-gat}的提出，为自然语言处理领域的研究者提供了更多将句法等结构化信息融入神经网络模型中的选择。


自2018年以来，随着以BERT为代表的预训练模型在自然语言处理领域逐渐占据主流地位，其强大的表示和抽象能力使得结构化信息的作用开始受到质疑。
研究者开始设计方法对BERT中是否已经隐含句法等结构信息进行探测。
其中，Hewitt等人\cite{hewitt-manning-2019-structural}于2019年提出了计算BERT经过线性映射之后不同词之间距离的方法，并发现BERT中隐含有句中词在句法依存树上的距离信息。
而Jawahar等人\cite{jawahar-etal-2019-bert}和Lin等人\cite{lin-etal-2019-open}在同年发现，BERT的较低层的表示隐含更多句法信息，而较高层的的表示隐含更多语义信息。

尽管结构化信息在预训练时代的必要性受到了质疑，仍有一系列工作在尝试将句法树等结构化信息与预训练模型进行结合。
Fei等人\cite{fei-etal-2020-retrofitting}于2020年提出一种基于多任务学习的方法，在训练目标任务的同时预测句中词在句法树中的距离，隐式地将句法信息融入预训练模型中，从而提高了命名实体识别、情感分类、关系分类等任务上的性能。
而Munir等人\cite{munir-etal-2021-adaptive}于2021年提出一种使用Tree-LSTM和CNN将句法信息融入BERT顶层输出的方法，并将其应用到语义角色标注任务上，取得了较好的结果。

在另一方面，由于图结构的语义表示是一个新兴课题，目前相关工作大部分都集中于语义图数据库的构建\cite{banarescu-etal-2013-abstract,abend-rappoport-2013-universal,oepen-etal-2015-semeval,che-etal-2016-semeval}和语义图分析方法的研究\cite{hershcovich-etal-2017-transition,dozat-manning-2018-simpler,cai-lam-2020-amr}。
在语义相关的结构化信息应用上目前仅有少量研究。
其中，Zhang等人\cite{zhang-etal-2020-semantics}于2020年提出将语义角色标注标签转化为表示向量，与从BERT中获取的表示向量合并，从而将语义角色标注信息融入BERT表示。
%他们将该方法应用于文本分类、自然语言推理和语义相似性任务上，并取得了明显的性能提升。
而Wu等人\cite{wu-etal-2021-infusing}于2021年提出使用图卷积网络\cite{kipf-welling-2017-semi}（Graph Convolutional Network，简称GCN）将语义依存信息融入BERT顶层输出中，并在文本分类、自然语言推理和语义相似性任务上验证了该方法的有效性。

现有的方法大部分都将预训练模型作为一个独立的词表示抽取器，仅仅使用它们获得词的表示向量，然后再使用与传统神经网络模型中相似的方法将结构化信息融入表示，并没有将结构化信息与预训练模型很好地结合。
此外，与大部分其他语言的预训练模型使用词片段作为基本输入单位不同，中文的预训练模型多使用字作为基本输入单位。
而由于此前方法首先用预训练模型获取词向量，然后在词向量基础上加入词级别的结构信息，在中文中很重要的字级别结构信息就被忽略了。

为了解决上述两个问题，本章提出基于字级别依存关系的强化预训练模型，首先将传统的词级别依存关系转化为字级别，然后利用图神经网络将这些结构融入预训练模型的每一层，从而将结构信息与预训练模型紧密结合在一起，获取了结构信息强化的预训练模型。
为了验证该方法的有效性，本章首先在中文语义角色标注任务上进行了实验，分别尝试将句法依存树信息和语义依存图信息融入预训练模型中，从而提高模型在该任务上的性能。
实验结果表明这两类信息都能有效提高模型性能，且建模的是不同类型信息，二者结合在一起时能进一步提高模型性能。
另外，本章还在中文关系抽取任务上进行了实验，进一步证明了语义依存图信息的融入对于模型性能提高的帮助。
本章提出的模型在这两个任务上都取得了目前最好结果。


\section{背景与相关工作}[Related Work]

语义角色标注任务于2000年被Gildea等人最先提出，在此后的二十年中，研究者一直致力于使用结构化的句法信息帮助该任务，并取得了显著效果。
而这些研究中的宝贵经验，对于实现本文的利用语义依存图信息帮助其他任务的目标具有很强的借鉴意义。
因此，本节首先介绍语义角色标注领域发展过程中结构化句法信息应用方法的演变，然后具体介绍其中近年来较常用的使用图卷积网络将结构信息融入神经网络模型的方法。

在语义角色标注研究的早期阶段，研究者普遍采用基于特征工程的方法\cite{zhao-etal-2009-multilingual-dependency,bjorkelund-etal-2009-multilingual,li-etal-2009-improving}。
在这一阶段，句法信息一般被用作额外的特征用于增强模型\cite{pradhan-etal-2005-semantic-role,che-etal-2006-hybrid}。
此后，随着神经网络模型在自然语言处理领域的广泛应用，该领域涌现出了一批不使用句法信息的（syntax-agnostic）基于神经网络的语义角色标注模型，获取了比使用句法信息的（syntax-aware）特征工程方法更好的结果。
其中，Fitzgerald等人\cite{fitzgerald-etal-2015-semantic}在2015年提出使用神经网络同时对论元及其标签进行建模。
Zhou等人\cite{zhou-xu-2015-end}于2015年将双向LSTM应用到基于词组的（span-based）语义角色标注中并取得了较好结果。
类似地，Marcheggiani等人\cite{marcheggiani-etal-2017-simple}将双向LSTM应用到基于词的（word-based）语义角色标注中并提升了其性能。
随着这些模型的提出，研究者开始讨论在深度学习时代句法信息是否还对语义角色标注有帮助。
而这种讨论又引出了一系列在神经网络框架下利用句法信息帮助语义角色标注的工作。
其中，Roth等人\cite{roth-lapata-2016-neural}在2016年提出了将句法树上的路径转化为向量向神经网络模型中加入句法信息的方法。
在2017年，Marcheggiani等人\cite{marcheggiani-titov-2017-encoding}和Qian等人\cite{qian-etal-2017-syntax}分别提出了使用图卷积网络和树结构LSTM对句法信息直接建模的方法。
沿着他们的脚步，Li等人\cite{li-etal-2018-unified}于2018年尝试了多种将句法信息融入神经网络模型的方法。
而Munir等人\cite{munir-etal-2021-adaptive}于2021年提出使用适应性卷积网络（adaptive convolution network）和Tree-LSTM将句法信息直接融入神经网络。

除了上述显式地将句法信息融入神经网络的工作外，还有一批工作尝试使用其他方式利用句法信息帮助语义角色标注。
例如，He等人\cite{he-etal-2018-syntax,he-etal-2019-syntax}尝试使用句法信息帮助模型进行论元剪枝，删除不可能的论元从而提高预测准确率。
Strubell等人\cite{strubell-etal-2018-linguistically}和Xia等人\cite{xia-etal-2019-syntax}分别提出了使用多任务学习框架同时学习句法分析任务和语义角色标注，从而隐式地将句法信息通过共享的参数传递给语义角色标注任务。

近年来，随着BERT等预训练模型在自然语言处理各领域取得重大成功，语义角色标注领域也出现了使用预训练模型的工作。
例如，Shi等人\cite{shi-lin-2019-simple}在BERT输出的基础上使用一个简单的序列标注框架用于解决语义角色标注任务，取得了显著优于此前最好结果的性能，为预训练时代的语义角色标注提供了一个很好的基线模型。
此后，Li等人\cite{li-etal-2020-high}在预训练模型的基础上将高维图结构特征引入模型中，进一步提高了模型性能。
与此同时，在预训练时代，结构化句法信息对于语义角色标注任务的帮助又受到了质疑。
正如第\ref{sec:chapter5-intro}节中介绍的，一系列针对BERT的探测任务表明其中已经隐含了结构化句法信息。
另一方面，在此前的使用句法信息的语义角色标注工作中，也有研究者发现当使用BERT代替传统上下文无关词向量作为模型输入时，加入句法信息带来的性能提升变得十分有限\cite{xia-etal-2019-syntax}。
这种发现也说明将结构化信息与预训练模型融合是十分具有挑战性的工作。

纵观语义角色标注领域的发展史，可以发现其对结构化句法信息的使用呈螺旋上升态势，即在每个新的时代开始时，随着更强模型的提出，句法信息的有效性就会受到质疑。
但之后随着新的建模结构化信息的方法的提出，又会证明句法信息仍然能有效地帮助语义角色标注。
因此本章以使用语义依存图帮助其他任务为目标，致力于解决结构化信息和预训练模型的融合问题。
本章针对中文预训练模型以字为基本输入单位的特点，提出了基于字级别依存关系的强化预训练模型，并在中文语义角色标注和关系抽取任务上验证了其有效性。


\section{基于字级别依存关系的预训练语言模型增强方法}[Pre-trained Model Enhancement Based on Character-Level Dependency Relations]


\section{依存关系增强的预训练语言模型的应用}[Application of Relation-Enhanced Pre-trained Models]

\subsection{基于预训练语言模型的命名实体识别模型}[Semantic Role Labeling Model Based on Pre-trained Models]

\subsection{基于预训练语言模型的关系抽取模型}[Relation Extraction Model Based on Pre-trained Models]


\section{实验与分析}[Experiments and Analysis]

\subsection{实验设置}[Experimental Settings]


\section{本章小结}[Conclusions]


% Local Variables:
% TeX-master: "../thesis"
% TeX-engine: xetex
% End: